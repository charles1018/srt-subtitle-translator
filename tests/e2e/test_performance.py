"""E2E æ¸¬è©¦ - æ•ˆèƒ½èˆ‡é‚Šç·£æƒ…æ³æ¸¬è©¦

æ­¤æ¨¡çµ„æ¸¬è©¦ç³»çµ±æ•ˆèƒ½èˆ‡é‚Šç·£æƒ…æ³è™•ç†ï¼ŒåŒ…æ‹¬ï¼š
1. æ•ˆèƒ½èˆ‡æ•ˆç‡æ¸¬è©¦ï¼ˆ2-3 å€‹æ¸¬è©¦ï¼‰
2. é‚Šç·£æƒ…æ³æ¸¬è©¦ï¼ˆ3 å€‹æ¸¬è©¦ï¼‰

å°æ‡‰éšæ®µä¸‰ä»»å‹™ 4 - æ‰¹é‡è™•ç†èˆ‡æ•ˆèƒ½æ¸¬è©¦
"""

import time
from pathlib import Path
from unittest.mock import AsyncMock, Mock

import pysrt
import pytest

from srt_translator.services.factory import ServiceFactory

# ============================================================
# æ¸¬è©¦å‰æº–å‚™ï¼šMock æ‰€æœ‰æœå‹™
# ============================================================


@pytest.fixture(autouse=True)
def reset_service_factory():
    """åœ¨æ¯å€‹æ¸¬è©¦å‰é‡ç½® ServiceFactory"""
    ServiceFactory._instances.clear()
    yield
    ServiceFactory._instances.clear()


@pytest.fixture
def mock_all_services_for_performance(mock_translation_client, mock_translation_responses):
    """ç‚ºæ•ˆèƒ½æ¸¬è©¦æº–å‚™å®Œæ•´çš„ Mock æœå‹™"""

    # Mock TranslationService çš„ç¿»è­¯é‚è¼¯
    async def mock_translate_text(text, context, llm_type, model):
        return mock_translation_responses.get(text, f"[Mockç¿»è­¯] {text}")

    async def mock_translate_batch(texts_with_context, llm_type, model, concurrent_limit=5):
        return [mock_translation_responses.get(text, f"[Mockç¿»è­¯] {text}") for text, context in texts_with_context]

    # å»ºç«‹ Mock TranslationService
    mock_translation_service = Mock()
    mock_translation_service.translate_text = AsyncMock(side_effect=mock_translate_text)
    mock_translation_service.translate_batch = AsyncMock(side_effect=mock_translate_batch)
    mock_translation_service.cleanup = Mock()
    mock_translation_service.stats = {
        "total_translations": 0,
        "cached_translations": 0,
        "failed_translations": 0,
    }

    # Mock ModelService
    mock_model_service = Mock()
    mock_model_service.get_translation_client = AsyncMock(return_value=mock_translation_client)
    mock_model_service.api_keys = {"openai": "test-key", "ollama": ""}
    mock_model_service.cleanup = AsyncMock()

    # Mock CacheServiceï¼ˆå¯è¿½è¹¤å¿«å–è¡Œç‚ºï¼‰
    cache_storage = {}
    cache_hits = []

    def mock_get_translation(source_text, context_texts, model_name):
        key = f"{source_text}:{model_name}"
        result = cache_storage.get(key)
        if result:
            cache_hits.append(key)
        return result

    def mock_store_translation(source_text, target_text, context_texts, model_name):
        key = f"{source_text}:{model_name}"
        cache_storage[key] = target_text
        return True

    mock_cache_service = Mock()
    mock_cache_service.get_translation = Mock(side_effect=mock_get_translation)
    mock_cache_service.store_translation = Mock(side_effect=mock_store_translation)
    mock_cache_service.cleanup = Mock()
    mock_cache_service._storage = cache_storage
    mock_cache_service._hits = cache_hits

    # Mock FileService
    mock_file_service = Mock()
    mock_file_service.get_output_path = Mock()
    mock_file_service.cleanup = Mock()

    # Mock ProgressService
    mock_progress_service = Mock()
    mock_progress_service.register_progress_callback = Mock()
    mock_progress_service.register_complete_callback = Mock()
    mock_progress_service.set_total = Mock()
    mock_progress_service.increment_progress = Mock()
    mock_progress_service.cleanup = Mock()

    # è¨»å†Šåˆ° ServiceFactory
    ServiceFactory._instances = {
        "TranslationService": mock_translation_service,
        "ModelService": mock_model_service,
        "CacheService": mock_cache_service,
        "FileService": mock_file_service,
        "ProgressService": mock_progress_service,
    }

    yield {
        "translation": mock_translation_service,
        "model": mock_model_service,
        "cache": mock_cache_service,
        "file": mock_file_service,
        "progress": mock_progress_service,
    }


# ============================================================
# æ•ˆèƒ½èˆ‡æ•ˆç‡æ¸¬è©¦ï¼ˆ2 å€‹æ¸¬è©¦ï¼‰
# ============================================================


@pytest.mark.asyncio
async def test_translation_speed(sample_srt_path: Path, mock_all_services_for_performance):
    """æ¸¬è©¦ 1ï¼šç¿»è­¯é€Ÿåº¦æ¸¬è©¦

    é©—è­‰ï¼š
    1. æ¸¬é‡å–®å€‹å­—å¹•ç¿»è­¯æ™‚é–“
    2. ç¿»è­¯é€Ÿåº¦åœ¨åˆç†ç¯„åœå…§ï¼ˆ< 2 ç§’ï¼‰
    3. æ‰¹é‡ç¿»è­¯æ¯”å–®å€‹å¿«
    """
    # æº–å‚™
    translation_service = ServiceFactory.get_translation_service()

    # è®€å–è¼¸å…¥æª”æ¡ˆ
    input_subs = pysrt.open(str(sample_srt_path), encoding="utf-8")
    assert len(input_subs) == 3, "æ¸¬è©¦æª”æ¡ˆæ‡‰æœ‰ 3 å€‹å­—å¹•"

    # æ¸¬è©¦ 1ï¼šå–®å€‹ç¿»è­¯é€Ÿåº¦
    single_times = []
    for sub in input_subs:
        start_time = time.time()
        translation = await translation_service.translate_text(sub.text, [sub.text], "openai", "test-model")
        duration = time.time() - start_time
        single_times.append(duration)

    # é©—è­‰å–®å€‹ç¿»è­¯é€Ÿåº¦
    avg_single_time = sum(single_times) / len(single_times)
    assert avg_single_time < 2.0, f"å–®å€‹ç¿»è­¯å¹³å‡è€—æ™‚ {avg_single_time:.3f}sï¼Œè¶…é 2 ç§’é™åˆ¶"

    # æ¸¬è©¦ 2ï¼šæ‰¹é‡ç¿»è­¯é€Ÿåº¦
    texts_with_context = [(sub.text, [sub.text]) for sub in input_subs]

    start_time = time.time()
    batch_translations = await translation_service.translate_batch(
        texts_with_context, "openai", "test-model", concurrent_limit=3
    )
    batch_time = time.time() - start_time

    # é©—è­‰æ‰¹é‡ç¿»è­¯é€Ÿåº¦
    assert batch_time < 3.0, f"æ‰¹é‡ç¿»è­¯è€—æ™‚ {batch_time:.3f}sï¼Œè¶…é 3 ç§’é™åˆ¶"

    # é©—è­‰æ‰¹é‡ç¿»è­¯æœ‰æ•ˆç‡å„ªå‹¢ï¼ˆæ‡‰è©²æ›´å¿«æˆ–æ¥è¿‘ï¼‰
    total_single_time = sum(single_times)
    assert batch_time <= total_single_time * 1.5, (
        f"æ‰¹é‡ç¿»è­¯æ™‚é–“ {batch_time:.3f}s æ‡‰è©²ä¸è¶…éå–®å€‹ç¸½æ™‚é–“ {total_single_time:.3f}s çš„ 1.5 å€"
    )


@pytest.mark.asyncio
async def test_cache_performance(sample_srt_path: Path, mock_all_services_for_performance):
    """æ¸¬è©¦ 2ï¼šå¿«å–æ•ˆèƒ½æ¸¬è©¦

    é©—è­‰ï¼š
    1. æ¯”è¼ƒæœ‰å¿«å–å’Œç„¡å¿«å–çš„é€Ÿåº¦
    2. å¿«å–å‘½ä¸­ç‡
    3. å¿«å–é¡¯è‘—æå‡æ•ˆèƒ½
    """
    # æº–å‚™
    translation_service = ServiceFactory.get_translation_service()
    cache_service = ServiceFactory.get_cache_service()

    # è®€å–è¼¸å…¥æª”æ¡ˆ
    input_subs = pysrt.open(str(sample_srt_path), encoding="utf-8")
    test_text = input_subs[0].text
    model_name = "test-model"

    # ç¬¬ä¸€æ¬¡ç¿»è­¯ï¼ˆç„¡å¿«å–ï¼‰
    start_time = time.time()
    result1 = await translation_service.translate_text(test_text, [test_text], "openai", model_name)
    first_time = time.time() - start_time

    # å„²å­˜åˆ°å¿«å–
    cache_service.store_translation(test_text, result1, [test_text], model_name)

    # ç¬¬äºŒæ¬¡ç¿»è­¯ï¼ˆæœ‰å¿«å–ï¼‰
    start_time = time.time()
    cached = cache_service.get_translation(test_text, [test_text], model_name)
    cache_time = time.time() - start_time

    # é©—è­‰å¿«å–å‘½ä¸­
    assert cached is not None, "å¿«å–æ‡‰è©²å‘½ä¸­"
    assert cached == result1, "å¿«å–çµæœæ‡‰è©²èˆ‡ç¬¬ä¸€æ¬¡ç¿»è­¯ç›¸åŒ"

    # é©—è­‰å¿«å–é€Ÿåº¦å„ªå‹¢ï¼ˆå¿«å–è®€å–æ‡‰è©²æ›´å¿«ï¼‰
    assert cache_time <= first_time, f"å¿«å–è®€å–æ™‚é–“ {cache_time:.6f}s æ‡‰è©²ä¸è¶…éé¦–æ¬¡ç¿»è­¯æ™‚é–“ {first_time:.6f}s"

    # æ¸¬è©¦å¤šæ¬¡å¿«å–å‘½ä¸­
    cache_service._hits.clear()  # é‡ç½®å¿«å–å‘½ä¸­è¨˜éŒ„

    for _ in range(5):
        cached = cache_service.get_translation(test_text, [test_text], model_name)
        assert cached == result1, "æ¯æ¬¡å¿«å–å‘½ä¸­æ‡‰è©²è¿”å›ç›¸åŒçµæœ"

    # é©—è­‰å¿«å–å‘½ä¸­æ¬¡æ•¸
    assert len(cache_service._hits) == 5, "æ‡‰è©²æœ‰ 5 æ¬¡å¿«å–å‘½ä¸­"


# ============================================================
# é‚Šç·£æƒ…æ³æ¸¬è©¦ï¼ˆ3 å€‹æ¸¬è©¦ï¼‰
# ============================================================


@pytest.mark.asyncio
async def test_large_file_processing(very_large_srt_path: Path, e2e_temp_dir: Path, mock_all_services_for_performance):
    """æ¸¬è©¦ 3ï¼šå¤§å‹æª”æ¡ˆè™•ç†

    é©—è­‰ï¼š
    1. ç¿»è­¯è¶…å¤§ SRT æª”æ¡ˆï¼ˆ100+ å­—å¹•ï¼‰
    2. èƒ½å¤ å®Œæ•´è™•ç†
    3. æ•ˆèƒ½å¯æ¥å—
    """
    # æº–å‚™
    translation_service = ServiceFactory.get_translation_service()

    # è®€å–å¤§å‹æª”æ¡ˆ
    input_subs = pysrt.open(str(very_large_srt_path), encoding="utf-8")
    assert len(input_subs) >= 100, "æ‡‰è©²æœ‰è‡³å°‘ 100 å€‹å­—å¹•"

    # æº–å‚™æ‰¹é‡ç¿»è­¯è³‡æ–™
    texts_with_context = [(sub.text, [sub.text]) for sub in input_subs]

    # æ‰¹é‡ç¿»è­¯å¤§å‹æª”æ¡ˆ
    start_time = time.time()
    translations = await translation_service.translate_batch(
        texts_with_context, "openai", "test-model", concurrent_limit=5
    )
    duration = time.time() - start_time

    # é©—è­‰å®Œæ•´æ€§
    assert len(translations) == len(input_subs), "ç¿»è­¯æ•¸é‡æ‡‰è©²èˆ‡å­—å¹•æ•¸é‡ä¸€è‡´"
    assert all(isinstance(t, str) for t in translations), "æ‰€æœ‰ç¿»è­¯éƒ½æ‡‰è©²æ˜¯å­—ä¸²"
    assert all(len(t) > 0 for t in translations), "æ‰€æœ‰ç¿»è­¯éƒ½ä¸æ‡‰è©²ç‚ºç©º"

    # é©—è­‰æ•ˆèƒ½ï¼ˆMock æ‡‰è©²å¾ˆå¿«ï¼Œä½†ä»è¦æœ‰åˆç†é™åˆ¶ï¼‰
    assert duration < 30.0, f"å¤§å‹æª”æ¡ˆè™•ç†è€—æ™‚ {duration:.2f}sï¼Œè¶…é 30 ç§’é™åˆ¶"

    # é©—è­‰ç¬¬ä¸€å€‹å’Œæœ€å¾Œä¸€å€‹ç¿»è­¯æ­£ç¢º
    assert "é€™æ˜¯å­—å¹•ç·¨è™Ÿ 1" in translations[0], "ç¬¬ä¸€å€‹ç¿»è­¯æ‡‰è©²æ­£ç¢º"
    assert "é€™æ˜¯å­—å¹•ç·¨è™Ÿ" in translations[-1], "æœ€å¾Œä¸€å€‹ç¿»è­¯æ‡‰è©²æ­£ç¢º"


@pytest.mark.asyncio
async def test_long_subtitle_processing(
    long_subtitle_srt_path: Path, mock_all_services_for_performance, mock_translation_responses
):
    """æ¸¬è©¦ 4ï¼šæ¥µé•·å­—å¹•è™•ç†

    é©—è­‰ï¼š
    1. å­—å¹•æ–‡æœ¬è¶…é•·ï¼ˆ1000+ å­—ç¬¦ï¼‰
    2. æ­£ç¢ºè™•ç†
    3. ç¿»è­¯å®Œæ•´æ€§
    """
    # æº–å‚™
    translation_service = ServiceFactory.get_translation_service()

    # è®€å–åŒ…å«è¶…é•·å­—å¹•çš„æª”æ¡ˆ
    input_subs = pysrt.open(str(long_subtitle_srt_path), encoding="utf-8")
    assert len(input_subs) >= 3, "æ‡‰è©²æœ‰è‡³å°‘ 3 å€‹å­—å¹•"

    # é©—è­‰ç¬¬ä¸€å€‹å­—å¹•è¶…é•·
    first_subtitle = input_subs[0].text
    assert len(first_subtitle) > 1000, "ç¬¬ä¸€å€‹å­—å¹•æ‡‰è©²è¶…é 1000 å­—ç¬¦"

    # ç¿»è­¯è¶…é•·å­—å¹•
    start_time = time.time()
    long_translation = await translation_service.translate_text(
        first_subtitle, [first_subtitle], "openai", "test-model"
    )
    long_duration = time.time() - start_time

    # é©—è­‰è¶…é•·å­—å¹•ç¿»è­¯
    assert long_translation, "è¶…é•·å­—å¹•æ‡‰è©²èƒ½å¤ ç¿»è­¯"
    assert len(long_translation) > 0, "ç¿»è­¯çµæœä¸æ‡‰è©²ç‚ºç©º"

    # é©—è­‰é€Ÿåº¦åˆç†ï¼ˆå³ä½¿æ˜¯è¶…é•·å­—å¹•ï¼‰
    assert long_duration < 5.0, f"è¶…é•·å­—å¹•ç¿»è­¯è€—æ™‚ {long_duration:.2f}sï¼Œè¶…é 5 ç§’é™åˆ¶"

    # ç¿»è­¯æ™®é€šå­—å¹•ï¼ˆç¬¬äºŒå€‹å­—å¹•ï¼‰
    normal_subtitle = input_subs[1].text
    assert len(normal_subtitle) < 100, "ç¬¬äºŒå€‹å­—å¹•æ‡‰è©²æ˜¯æ™®é€šé•·åº¦"

    normal_translation = await translation_service.translate_text(
        normal_subtitle, [normal_subtitle], "openai", "test-model"
    )

    # é©—è­‰æ™®é€šå­—å¹•ç¿»è­¯
    assert normal_translation == mock_translation_responses[normal_subtitle], "æ™®é€šå­—å¹•ç¿»è­¯æ‡‰è©²æ­£ç¢º"


@pytest.mark.asyncio
async def test_special_characters_processing(special_chars_srt_path: Path, mock_all_services_for_performance):
    """æ¸¬è©¦ 5ï¼šç‰¹æ®Šå­—ç¬¦è™•ç†

    é©—è­‰ï¼š
    1. å­—å¹•åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼ˆemojiã€ç¬¦è™Ÿã€å¤šèªè¨€å­—ç¬¦ï¼‰
    2. ç‰¹æ®Šå­—ç¬¦ä¿ç•™
    3. ä¸æœƒå°è‡´éŒ¯èª¤
    """
    # æº–å‚™
    translation_service = ServiceFactory.get_translation_service()

    # è®€å–åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„æª”æ¡ˆ
    input_subs = pysrt.open(str(special_chars_srt_path), encoding="utf-8")
    assert len(input_subs) >= 5, "æ‡‰è©²æœ‰è‡³å°‘ 5 å€‹å­—å¹•"

    # ç¿»è­¯æ‰€æœ‰åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„å­—å¹•
    translations = []
    for sub in input_subs:
        translation = await translation_service.translate_text(sub.text, [sub.text], "openai", "test-model")
        translations.append(
            {
                "original": sub.text,
                "translation": translation,
            }
        )

    # é©—è­‰ç¿»è­¯çµæœ
    assert len(translations) == len(input_subs), "æ‰€æœ‰å­—å¹•éƒ½æ‡‰è©²è¢«ç¿»è­¯"

    # é©—è­‰ç‰¹æ®Šå­—ç¬¦ä¿ç•™
    for result in translations:
        assert result["translation"], "ç¿»è­¯çµæœä¸æ‡‰è©²ç‚ºç©º"

        # æª¢æŸ¥ç‰¹å®šçš„ç‰¹æ®Šå­—ç¬¦æ˜¯å¦ä¿ç•™
        if "ğŸŒ" in result["original"]:
            assert "ğŸŒ" in result["translation"], "Emoji æ‡‰è©²è¢«ä¿ç•™"

        if "ğŸ˜€" in result["original"]:
            assert "ğŸ˜€" in result["translation"], "Emoji æ‡‰è©²è¢«ä¿ç•™"

        if "@#$%^&*" in result["original"]:
            assert "@#$%^&*" in result["translation"], "ç¬¦è™Ÿæ‡‰è©²è¢«ä¿ç•™"

    # é©—è­‰æ²’æœ‰æ‹‹å‡ºç•°å¸¸
    # å¦‚æœç¨‹å¼ç¢¼åŸ·è¡Œåˆ°é€™è£¡ï¼Œè¡¨ç¤ºç‰¹æ®Šå­—ç¬¦è™•ç†æ­£å¸¸ï¼Œæ²’æœ‰å´©æ½°
    assert True, "ç‰¹æ®Šå­—ç¬¦è™•ç†æ­£å¸¸"
